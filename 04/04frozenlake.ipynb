{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ff3d1",
   "metadata": {},
   "source": [
    "# Q-Learning on FrozenLake Environment (with Live Q-Table Visualization)\n",
    "\n",
    "This notebook demonstrates how to apply the Q-learning algorithm on the FrozenLake-v1 environment using OpenAI Gym.  \n",
    "We will train the agent, visualize the learning process, and finally test the learned policy with visual rendering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07be7d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time  # for delay during plot updates\n",
    "import gym  # OpenAI Gym environment\n",
    "import matplotlib.pyplot as plt  # for Q-table heatmap visualization\n",
    "import numpy as np  # numerical computations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc10a47",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We define two environments:\n",
    "- One for training (no rendering)\n",
    "- One for testing (with human-readable visualization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42edf3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "test_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8c16b",
   "metadata": {},
   "source": [
    "## Q-Table Initialization and Hyperparameters\n",
    "\n",
    "We initialize the Q-table with zeros and define the learning rate, discount factor, exploration rate, and total episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9f083",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "alpha = 0.3      # learning rate\n",
    "gamma = 0.99     # discount factor\n",
    "epsilon = 0.5    # exploration rate\n",
    "episodes = 3000  # total training episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9105d",
   "metadata": {},
   "source": [
    "## Q-Table Heatmap Setup\n",
    "\n",
    "We use matplotlib to visualize the agent's learning progress in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9c677",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(q_table, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(img)\n",
    "plt.title(\"Q-Table Heatmap (State x Action)\")\n",
    "\n",
    "def update_plot():\n",
    "    img.set_data(q_table)\n",
    "    ax.set_xlabel(\"Actions (0:Left, 1:Down, 2:Right, 3:Up)\")\n",
    "    ax.set_ylabel(\"States (0-15)\")\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb219ca",
   "metadata": {},
   "source": [
    "## Q-Learning Training Loop\n",
    "\n",
    "The agent will explore and learn which actions yield the most rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee0298",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # exploitation\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        update_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a76f6",
   "metadata": {},
   "source": [
    "## Final Q-Table Plot\n",
    "\n",
    "Let's show the final state of the Q-table after all episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6388ce4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "update_plot()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8487b",
   "metadata": {},
   "source": [
    "## Testing the Learned Policy\n",
    "\n",
    "We now test the learned policy in the same environment, but with rendering turned on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73618c6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "state = test_env.reset()[0]\n",
    "test_env.render()\n",
    "\n",
    "for _ in range(20):\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, _, _ = test_env.step(action)\n",
    "    test_env.render()\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
