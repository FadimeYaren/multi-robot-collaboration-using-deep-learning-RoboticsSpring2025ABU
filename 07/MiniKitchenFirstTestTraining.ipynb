{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3WkMnkA4oZzZ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "class KitchenEnv:\n",
        "    def __init__(self, grid, num_agents=2):\n",
        "        self.grid = grid\n",
        "        self.grid_width = len(grid[0])\n",
        "        self.grid_height = len(grid)\n",
        "        self.num_agents = num_agents\n",
        "\n",
        "        # Ajan baÅŸlangÄ±Ã§ konumlarÄ± (Ã¶rnek pozisyonlar)\n",
        "        self.agent_positions = [[1, 1 + i] for i in range(num_agents)]\n",
        "        self.agent_dirs = [\"move_down\"] * num_agents\n",
        "        self.agent_items = [None for _ in range(num_agents)]\n",
        "\n",
        "        # Ortam nesneleri (masa Ã¼stÃ¼ne bÄ±rakÄ±lan her ÅŸey)\n",
        "        self.objects_on_map = {}  # {(x, y): {\"type\": \"Tomato\", \"state\": \"Chopped\"}, ...}\n",
        "\n",
        "        # Loglama altyapÄ±sÄ±\n",
        "        self.agent_logs = {f\"agent_{i}\": [] for i in range(num_agents)}\n",
        "        self.burger_logs = []\n",
        "        self.snapshot_logs = []\n",
        "\n",
        "        # DiÄŸer durumlar\n",
        "        self.time_step = 0\n",
        "        self.terminated = False\n",
        "\n",
        "    def reset(self):\n",
        "        # TÃ¼m ortamÄ± yeniden baÅŸlat\n",
        "        self.agent_positions = [[1, 1 + i] for i in range(self.num_agents)]\n",
        "        self.agent_dirs = [\"move_down\"] * self.num_agents\n",
        "        self.agent_items = [None for _ in range(self.num_agents)]\n",
        "        self.objects_on_map = {}\n",
        "\n",
        "        self.agent_logs = {f\"agent_{i}\": [] for i in range(self.num_agents)}\n",
        "        self.burger_logs = []\n",
        "        self.snapshot_logs = []\n",
        "\n",
        "        self.time_step = 0\n",
        "        self.terminated = False\n",
        "\n",
        "        return [self.get_state(i) for i in range(self.num_agents)]\n",
        "\n",
        "    def get_state(self, agent_id):\n",
        "        # 1. Pozisyonu one-hot olarak encode et\n",
        "        pos = self.agent_positions[agent_id]\n",
        "        onehot_pos = [0] * (self.grid_width * self.grid_height)\n",
        "        idx = pos[1] * self.grid_width + pos[0]\n",
        "        onehot_pos[idx] = 1\n",
        "\n",
        "        # 2. Elindeki nesne\n",
        "        item = self.agent_items[agent_id]\n",
        "        item_types = [\"None\", \"Tomato\", \"Meat\", \"Lettuce\", \"Plate\"]\n",
        "        item_type = item[\"type\"] if item else \"None\"\n",
        "        onehot_item = [1 if item_type == t else 0 for t in item_types]\n",
        "\n",
        "        # 3. (isteÄŸe baÄŸlÄ±) daha sonra Ã§evresel bilgi veya direction eklenebilir\n",
        "\n",
        "        return onehot_pos + onehot_item\n",
        "\n",
        "    def step(self, actions):\n",
        "        rewards = [0.0 for _ in range(self.num_agents)]\n",
        "\n",
        "        for agent_id, action in enumerate(actions):\n",
        "            x, y = self.agent_positions[agent_id]\n",
        "            old_pos = (x, y)\n",
        "\n",
        "            if action == 0:  # Move Up\n",
        "                new_pos = (x, y - 1)\n",
        "            elif action == 1:  # Move Down\n",
        "                new_pos = (x, y + 1)\n",
        "            elif action == 2:  # Move Left\n",
        "                new_pos = (x - 1, y)\n",
        "            elif action == 3:  # Move Right\n",
        "                new_pos = (x + 1, y)\n",
        "            elif action == 4:  # Interact\n",
        "                rewards[agent_id] += self.handle_interact(agent_id)\n",
        "                new_pos = (x, y)\n",
        "            else:\n",
        "                new_pos = (x, y)  # invalid action\n",
        "\n",
        "            # Hareket geÃ§erli mi kontrol et\n",
        "            if (0 <= new_pos[0] < self.grid_width) and (0 <= new_pos[1] < self.grid_height):\n",
        "                self.agent_positions[agent_id] = list(new_pos)\n",
        "\n",
        "            # Log ekle\n",
        "            self.agent_logs[f\"agent_{agent_id}\"].append({\n",
        "                \"step\": self.time_step,\n",
        "                \"position\": self.agent_positions[agent_id],\n",
        "                \"action\": action,\n",
        "                \"item\": copy.deepcopy(self.agent_items[agent_id])\n",
        "            })\n",
        "\n",
        "        # Ortam snapshot'Ä± kaydet\n",
        "        self.snapshot_logs.append({\n",
        "            \"step\": self.time_step,\n",
        "            \"agents\": copy.deepcopy(self.agent_positions),\n",
        "            \"items\": copy.deepcopy(self.agent_items),\n",
        "            \"objects\": copy.deepcopy(self.objects_on_map)\n",
        "        })\n",
        "\n",
        "        self.time_step += 1\n",
        "        return [self.get_state(i) for i in range(self.num_agents)], rewards, self.is_done(), {}\n",
        "\n",
        "\n",
        "    def handle_interact(self, agent_id):\n",
        "        x, y = self.agent_positions[agent_id]\n",
        "        cell = self.grid[y][x]\n",
        "        item = self.agent_items[agent_id]\n",
        "        reward = 0.0\n",
        "\n",
        "        pos_key = (x, y)\n",
        "        ground_obj = self.objects_on_map.get(pos_key)\n",
        "\n",
        "        # ðŸš€ PICKUP from source stations\n",
        "        if item is None and cell in [\"T\", \"B\", \"TO\", \"M\", \"L\", \"PL\"]:\n",
        "            new_item = self.generate_item_from_station(cell)\n",
        "            if new_item:\n",
        "              self.agent_items[agent_id] = new_item\n",
        "              self.terminated = True  # âœ… pickup iÅŸlemi baÅŸarÄ± â†’ episode bitir\n",
        "              return 1.0  # âœ… yÃ¼ksek Ã¶dÃ¼l ver\n",
        "\n",
        "        # ðŸ“¦ PICKUP from table\n",
        "        if item is None and ground_obj:\n",
        "            self.agent_items[agent_id] = ground_obj\n",
        "            del self.objects_on_map[pos_key]\n",
        "            return 0.2\n",
        "\n",
        "        # ðŸªµ DROP to table\n",
        "        if item and cell == \"T\" and pos_key not in self.objects_on_map:\n",
        "            self.objects_on_map[pos_key] = item\n",
        "            self.agent_items[agent_id] = None\n",
        "            return 0.2\n",
        "\n",
        "        # ðŸ”ª CHOP\n",
        "        if item and cell == \"C\":\n",
        "            if item[\"type\"] in [\"Tomato\", \"Lettuce\"] and item[\"state\"] == \"Raw\":\n",
        "                item[\"state\"] = \"Chopped\"\n",
        "                return 0.5\n",
        "\n",
        "        # ðŸ”¥ COOK\n",
        "        if item and cell == \"P\":\n",
        "            if item[\"type\"] == \"Meat\" and item[\"state\"] == \"Raw\":\n",
        "                item[\"state\"] = \"Cooked\"\n",
        "                return 0.7\n",
        "\n",
        "        # ðŸ—‘ï¸ DISCARD\n",
        "        if item and cell == \"X\":\n",
        "            self.agent_items[agent_id] = None\n",
        "            return -0.2\n",
        "\n",
        "        # ðŸ” MERGE to plate on ground\n",
        "        if item and ground_obj:\n",
        "            merged = self.try_merge(item, ground_obj)\n",
        "            if merged:\n",
        "                self.objects_on_map[pos_key] = merged\n",
        "                self.agent_items[agent_id] = None\n",
        "                return 1.0\n",
        "\n",
        "        return -0.1  # geÃ§ersiz etkileÅŸim cezasÄ±\n",
        "\n",
        "\n",
        "    def generate_item_from_station(self, cell):\n",
        "        if cell == \"B\":\n",
        "            return {\"type\": \"Bread\", \"state\": \"Whole\"}\n",
        "        elif cell == \"M\":\n",
        "            return {\"type\": \"Meat\", \"state\": \"Raw\"}\n",
        "        elif cell == \"TO\":\n",
        "            return {\"type\": \"Tomato\", \"state\": \"Raw\"}\n",
        "        elif cell == \"L\":\n",
        "            return {\"type\": \"Lettuce\", \"state\": \"Raw\"}\n",
        "        elif cell == \"PL\":\n",
        "            return {\"type\": \"Plate\", \"state\": \"Clean\", \"contents\": []}\n",
        "        return None\n",
        "\n",
        "\n",
        "    def try_merge(self, item, plate):\n",
        "        if plate[\"type\"] != \"Plate\":\n",
        "            return None\n",
        "        if \"contents\" not in plate:\n",
        "            return None\n",
        "\n",
        "        # Zaten eklendiyse tekrar eklenmesin\n",
        "        if item[\"type\"] in plate[\"contents\"]:\n",
        "            return None\n",
        "\n",
        "        # Sadece belirli tÃ¼rler birleÅŸebilir\n",
        "        mergeable = [\"Tomato\", \"Lettuce\", \"Meat\", \"Bread\"]\n",
        "        if item[\"type\"] not in mergeable:\n",
        "            return None\n",
        "\n",
        "        # EÄŸer chop/cook durumu uygunsa devam\n",
        "        if item[\"type\"] in [\"Tomato\", \"Lettuce\"] and item[\"state\"] != \"Chopped\":\n",
        "            return None\n",
        "        if item[\"type\"] == \"Meat\" and item[\"state\"] != \"Cooked\":\n",
        "            return None\n",
        "\n",
        "        # Merge iÅŸlemi baÅŸarÄ±lÄ± â†’ iÃ§eriÄŸi gÃ¼ncelle\n",
        "        new_plate = copy.deepcopy(plate)\n",
        "        new_plate[\"contents\"].append(item[\"type\"])\n",
        "        new_plate[\"contents\"] = sorted(new_plate[\"contents\"])  # sÄ±ralÄ± olsun\n",
        "\n",
        "        # Yeni tabak tipi hesapla\n",
        "        new_plate[\"type\"] = self.get_plate_type(new_plate[\"contents\"])\n",
        "        return new_plate\n",
        "\n",
        "\n",
        "    def get_plate_type(self, contents):\n",
        "        if not contents:\n",
        "            return \"plate_clean\"\n",
        "        name = \"plate_\" + \"_\".join(c.lower() for c in contents)\n",
        "        if set(contents) == {\"Bread\", \"Tomato\", \"Lettuce\", \"Meat\"}:\n",
        "            return \"plate_burger\"  # final burger\n",
        "        return name\n",
        "\n",
        "\n",
        "    def is_done(self):\n",
        "      return self.terminated\n",
        "\n",
        "\n",
        "\n",
        "    def get_episode_log(self, episode_id):\n",
        "        return {\n",
        "            episode_id: {\n",
        "                \"agent_logs\": self.agent_logs,\n",
        "                \"burger_logs\": self.burger_logs,\n",
        "                \"snapshots\": [\n",
        "                    {\n",
        "                        \"step\": snap[\"step\"],\n",
        "                        \"agents\": snap[\"agents\"],\n",
        "                        \"items\": snap[\"items\"],\n",
        "                        \"objects\": {\n",
        "                            f\"{x},{y}\": v\n",
        "                            for (x, y), v in snap[\"objects\"].items()\n",
        "                        }\n",
        "                    }\n",
        "                    for snap in self.snapshot_logs\n",
        "                ]\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, gamma=0.95, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995, lr=0.001):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = self._build_model(lr)\n",
        "\n",
        "    def _build_model(self, lr):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(layers.Dense(64, activation='relu'))\n",
        "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(np.array([state]), verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def store(self, experience):\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def train(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        states, targets = [], []\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.amax(self.model.predict(np.array([next_state]), verbose=0)[0])\n",
        "            target_f = self.model.predict(np.array([state]), verbose=0)[0]\n",
        "            target_f[action] = target\n",
        "            states.append(state)\n",
        "            targets.append(target_f)\n",
        "\n",
        "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "8YfB5iPwoc2D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = [\n",
        "    [\"T\", \"B\", \"T\", \"T\", \"T\", \"T\", \"C\", \"P\", \"M\", \"T\"],\n",
        "    [\"T\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \"D\"],\n",
        "    [\"T\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \"PL\"],\n",
        "    [\"T\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \"P\"],\n",
        "    [\"T\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \"X\"],\n",
        "    [\"T\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\", \"T\"],\n",
        "    [\"T\", \"T\", \"T\", \"L\", \"TO\", \"T\", \"T\", \"C\", \"T\", \"T\"],\n",
        "]\n",
        "\n",
        "env = KitchenEnv(grid, num_agents=2)\n",
        "state_size = len(env.get_state(0))\n",
        "action_size = 5  # up, down, left, right, interact\n",
        "\n",
        "agents = [DQNAgent(state_size, action_size) for _ in range(2)]\n",
        "EPISODES = 3\n",
        "MAX_STEPS = 30\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "    states = env.reset()\n",
        "    done = False\n",
        "    total_reward = [0.0, 0.0]\n",
        "\n",
        "    for step in range(MAX_STEPS):\n",
        "        actions = [agents[i].select_action(states[i]) for i in range(2)]\n",
        "        next_states, rewards, done, _ = env.step(actions)\n",
        "\n",
        "        for i in range(2):\n",
        "            agents[i].store((states[i], actions[i], rewards[i], next_states[i], done))\n",
        "            agents[i].train()\n",
        "            total_reward[i] += rewards[i]\n",
        "\n",
        "        states = next_states\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode {ep+1}/{EPISODES} â€” Rewards: {total_reward}\")\n",
        "\n",
        "# Replay iÃ§in loglarÄ± kaydet\n",
        "logs = env.get_episode_log(f\"episode_{EPISODES}\")\n",
        "with open(\"episode_logs.json\", \"w\") as f:\n",
        "    import json\n",
        "    json.dump(logs, f, indent=2, default=str)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txRMAKcjoix8",
        "outputId": "34443590-0bcf-4fca-efaf-4b2acb3f62d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/3 â€” Rewards: [-0.6, -0.4]\n",
            "Episode 2/3 â€” Rewards: [-0.4, -0.6]\n",
            "Episode 3/3 â€” Rewards: [0.7, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUTBmPLE_nvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}